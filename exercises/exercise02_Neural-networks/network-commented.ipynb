{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9457ce6f-049b-44f3-9ed7-49b5c1318f16",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d107b093-d69f-4055-a9fb-c44bee638570",
   "metadata": {},
   "source": [
    "## 2 Linear Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760315eb-7a97-4d07-bdfb-9e1df737f760",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">\n",
    "The solution that we have for the question is the same: In our approach we also showed that a multi layer network is the same as a 1-layer network when using a linear activation function by repeating the operation recursively.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10758eda-c66a-4f7b-b0b4-608053d016e4",
   "metadata": {},
   "source": [
    "## 3 Programming a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f1e40a-bab5-4d74-afb1-d418a4eca3de",
   "metadata": {},
   "source": [
    "In this exercise a simple Multi-Layer Perceptron classifer is implemented using numpy. The python code below defines an MLP class with ReLU activations in the hidden layers and softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd57ca3-6013-4b3f-8232-1fc66cd30e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d42a932-bb34-4597-a776-6357570d838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "\n",
    "class ReLULayer(object):\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the ReLU of the input\n",
    "        # ReLu: returns the input if it's positive otherwise returns 0\n",
    "        relu = np.maximum(0, input)\n",
    "        return relu\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of ReLU from upstream_gradient and the stored input\n",
    "        # ( self.input > 0) is a binary mask, only passes input elements that had a positive value\n",
    "        downstream_gradient = upstream_gradient * ( self.input > 0)\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass # ReLU is parameter-free"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b9600-479d-4cad-83e6-2c35d7e72e4e",
   "metadata": {},
   "source": [
    "OutputLayer class:\n",
    "- the forward method computes the softmax of the input\n",
    "- the backward method computes the gradient of the loss with respect to the stored inputs. The gradient is computed by substracting 1 from the predicted posterior probabilities at the true label indices and divides them by the batch size. \n",
    "    (Zlk -1 if k = Yi*,\n",
    "    Zlk otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d119c3e6-2fae-46e6-833b-62aefe171485",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "\n",
    "class OutputLayer(object):\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the softmax of the input\n",
    "        input_exp = np.exp(input)\n",
    "        softmax = input_exp / np.sum(input_exp, axis = 1, keepdims=True) # your code here\n",
    "        return softmax\n",
    "\n",
    "    def backward(self, predicted_posteriors, true_labels):\n",
    "        # return the loss derivative with respect to the stored inputs\n",
    "        # use cross-entropy loss and the chain rule for softmax\n",
    "        batch_size = predicted_posteriors.shape[0]\n",
    "        downstream_gradient = predicted_posteriors.copy()\n",
    "        downstream_gradient[np.arange(batch_size), true_labels] -= 1\n",
    "        downstream_gradient /= batch_size\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass # softmax is parameter-free"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3433a59-1ef4-465d-b1ed-44f8c8e9ee09",
   "metadata": {},
   "source": [
    "LinearLayer class:\n",
    "- the forward method stores the input for later use and the preactivations are computed by taking the dot product of the input and weight and then addind the bias.\n",
    "    (Zin.B + b)\n",
    "- the backward method calculates the gradients for weights and biases using the upstream gradient from the next layer. The gradient for the biases is the sum of the upstream gradient along the first axis. The gradient of the weight is the dot product of the input transposed and the upstream gradient. The downstream gradient is the dot product of the upstream gradient and the transposed weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "595d92b7-26e6-4ec7-a65d-172cc9a10f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "\n",
    "class LinearLayer(object):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.n_inputs  = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        # randomly initialize weights and intercepts\n",
    "        # Weight matrix\n",
    "        self.B = np.random.normal(size = (n_inputs, n_outputs))\n",
    "        # Bias vector\n",
    "        self.b = np.random.normal(size = (1, n_outputs))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # compute the scalar product of input and weights\n",
    "        # (these are the preactivations for the subsequent non-linear layer)\n",
    "        preactivations = np.dot(input, self.B) + self.b\n",
    "        return preactivations\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of the weights from\n",
    "        # upstream_gradient and the stored input\n",
    "        # Gradient of bias\n",
    "        self.grad_b = np.sum(upstream_gradient, axis=0)\n",
    "        # Gradient of weights\n",
    "        self.grad_B = np.dot(self.input.T, upstream_gradient)\n",
    "        # compute the downstream gradient to be passed to the preceding layer\n",
    "        # Chain rule\n",
    "        downstream_gradient = np.dot(upstream_gradient, self.B.T)\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        # update the weights by batch gradient descent\n",
    "        self.B = self.B - learning_rate * self.grad_B\n",
    "        self.b = self.b - learning_rate * self.grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b9fbaf-1a20-461f-b497-74102d5cfc5b",
   "metadata": {},
   "source": [
    "MLP class:\n",
    "- the forward method reshapes input X to have dimensions (batch_size, n_features) and then iterates through each layer and computes forward pass by calling the forward method of each layer. The result of each layer is the input of the next layer.\n",
    "- the backward method performs the backpropagation by first calling the backward method of the last layer with predicted posteriors and true classes to get the upstream gradient. It iterates in reverse order through the layers and calls the backward method of the layers except the last one. The upstream gradient is updated at each layer and passed to the other layer until the backpropagation is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afd810df-3644-4a02-a176-c7fbcca83e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, n_features, layer_sizes):\n",
    "        # constuct a multi-layer perceptron\n",
    "        # with ReLU activation in the hidden layers and softmax output\n",
    "        # (i.e. it predicts the posterior probability of a classification problem)\n",
    "        #\n",
    "        # n_features: number of inputs\n",
    "        # len(layer_size): number of layers\n",
    "        # layer_size[k]: number of neurons in layer k\n",
    "        # (specifically: layer_sizes[-1] is the number of classes)\n",
    "        self.n_layers = len(layer_sizes)\n",
    "        self.layers   = []\n",
    "\n",
    "        # create interior layers (linear + ReLU)\n",
    "        n_in = n_features\n",
    "        for n_out in layer_sizes[:-1]:\n",
    "            self.layers.append(LinearLayer(n_in, n_out))\n",
    "            self.layers.append(ReLULayer())\n",
    "            n_in = n_out\n",
    "\n",
    "        # create last linear layer + output layer\n",
    "        n_out = layer_sizes[-1]\n",
    "        self.layers.append(LinearLayer(n_in, n_out))\n",
    "        self.layers.append(OutputLayer(n_out))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X is a mini-batch of instances\n",
    "        batch_size = X.shape[0]\n",
    "        # flatten the other dimensions of X (in case instances are images)\n",
    "        X = X.reshape(batch_size, -1)\n",
    "\n",
    "        # compute the forward pass\n",
    "        # (implicitly stores internal activations for later backpropagation)\n",
    "        result = X\n",
    "        for layer in self.layers:\n",
    "            result = layer.forward(result)\n",
    "        return result\n",
    "\n",
    "    def backward(self, predicted_posteriors, true_classes):\n",
    "        # perform backpropagation w.r.t. the prediction for the latest mini-batch X\n",
    "        upstream_gradient = self.layers[-1].backward(predicted_posteriors, true_classes)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            upstream_gradient = layer.backward(upstream_gradient)\n",
    "    \n",
    "    def update(self, X, Y, learning_rate):\n",
    "        posteriors = self.forward(X)\n",
    "        self.backward(posteriors, Y)\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "\n",
    "    def train(self, x, y, n_epochs, batch_size, learning_rate):\n",
    "        N = len(x)\n",
    "        n_batches = N // batch_size\n",
    "        for i in range(n_epochs):\n",
    "            # print(\"Epoch\", i)\n",
    "            # reorder data for every epoch\n",
    "            # (i.e. sample mini-batches without replacement)\n",
    "            permutation = np.random.permutation(N)\n",
    "\n",
    "            for batch in range(n_batches):\n",
    "                # create mini-batch\n",
    "                start = batch * batch_size\n",
    "                x_batch = x[permutation[start:start+batch_size]]\n",
    "                y_batch = y[permutation[start:start+batch_size]]\n",
    "\n",
    "                # perform one forward and backward pass and update network parameters\n",
    "                self.update(x_batch, y_batch, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48d7e09e-aa90-41f7-adf5-1bc04e06084d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate: 0.1965\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    # set training/test set size\n",
    "    N = 2000\n",
    "\n",
    "    # create training and test data\n",
    "    X_train, Y_train = datasets.make_moons(N, noise=0.05)\n",
    "    X_test,  Y_test  = datasets.make_moons(N, noise=0.05)\n",
    "    n_features = 2\n",
    "    n_classes  = 2\n",
    "\n",
    "    # standardize features to be in [-1, 1]\n",
    "    offset  = X_train.min(axis=0)\n",
    "    scaling = X_train.max(axis=0) - offset\n",
    "    X_train = ((X_train - offset) / scaling - 0.5) * 2.0\n",
    "    X_test  = ((X_test  - offset) / scaling - 0.5) * 2.0\n",
    "\n",
    "    # set hyperparameters (play with these!)\n",
    "    layer_sizes = [5, 5, n_classes]\n",
    "    n_epochs = 5\n",
    "    batch_size = 200\n",
    "    learning_rate = 0.05\n",
    "\n",
    "    # create network\n",
    "    network = MLP(n_features, layer_sizes)\n",
    "\n",
    "    # train\n",
    "    network.train(X_train, Y_train, n_epochs, batch_size, learning_rate)\n",
    "\n",
    "    # test\n",
    "    predicted_posteriors = network.forward(X_test)\n",
    "    # determine class predictions from posteriors by winner-takes-all rule\n",
    "    predicted_classes = np.argmax(predicted_posteriors, axis=1)\n",
    "    # compute and output the error rate of predicted_classes\n",
    "    error_rate = 1-np.sum(predicted_classes == Y_test) / len(Y_test)\n",
    "    print(\"error rate:\", error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e446337-4fcc-4a77-b779-f297eb2ca583",
   "metadata": {
    "tags": []
   },
   "source": [
    "Compare the validation errors on the dataset generated by the make_moons() function for the following four networks:\n",
    "- MLP(n_features, [2, 2, n_classes]) -> 2 hidden layers, each with 2 neurons.\n",
    "- MLP(n_features, [3, 3, n_classes]) -> 2 hidden layers, each with 3 neurons.\n",
    "- MLP(n_features, [5, 5, n_classes]) -> 2 hidden layers, each with 5 neurons.\n",
    "- MLP(n_features, [30, 30, n_classes]) -> 2 hidden layers, each with 30 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8aaa497-ebd6-49b1-a164-90f6890f0101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Errors:\n",
      "MLP([2, 2, n_classes]): 0.16000000000000003\n",
      "MLP([3, 3, n_classes]): 0.20199999999999996\n",
      "MLP([5, 5, n_classes]): 0.10999999999999999\n",
      "MLP([30, 30, n_classes]): 0.027000000000000024\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    # set training/test set size\n",
    "    N = 2000\n",
    "\n",
    "    # create training and test data\n",
    "    X_train, Y_train = datasets.make_moons(N, noise=0.05)\n",
    "    X_test,  Y_test  = datasets.make_moons(N, noise=0.05)\n",
    "    n_features = 2\n",
    "    n_classes  = 2\n",
    "\n",
    "    # standardize features to be in [-1, 1]\n",
    "    offset  = X_train.min(axis=0)\n",
    "    scaling = X_train.max(axis=0) - offset\n",
    "    X_train = ((X_train - offset) / scaling - 0.5) * 2.0\n",
    "    X_test  = ((X_test  - offset) / scaling - 0.5) * 2.0\n",
    "\n",
    "    # set hyperparameters (play with these!)\n",
    "    layer_sizes = [2, 2, n_classes]  # MLP(n_features, [2, 2, n_classes])\n",
    "    layer_sizes2 = [3, 3, n_classes]  # MLP(n_features, [3, 3, n_classes])\n",
    "    layer_sizes3 = [5, 5, n_classes]  # MLP(n_features, [5, 5, n_classes])\n",
    "    layer_sizes4 = [30, 30, n_classes]  # MLP(n_features, [30, 30, n_classes])\n",
    "\n",
    "    n_epochs = 5\n",
    "    batch_size = 200\n",
    "    learning_rate = 0.05\n",
    "\n",
    "    # create network\n",
    "    network = MLP(n_features, layer_sizes)\n",
    "    network2 = MLP(n_features, layer_sizes2)\n",
    "    network3 = MLP(n_features, layer_sizes3)\n",
    "    network4 = MLP(n_features, layer_sizes4)\n",
    "\n",
    "    # train\n",
    "    network.train(X_train, Y_train, n_epochs, batch_size, learning_rate)\n",
    "    network2.train(X_train, Y_train, n_epochs, batch_size, learning_rate)\n",
    "    network3.train(X_train, Y_train, n_epochs, batch_size, learning_rate)\n",
    "    network4.train(X_train, Y_train, n_epochs, batch_size, learning_rate)\n",
    "\n",
    "    # test\n",
    "    predicted_posteriors = network.forward(X_test)\n",
    "    predicted_posteriors2 = network2.forward(X_test)\n",
    "    predicted_posteriors3 = network3.forward(X_test)\n",
    "    predicted_posteriors4 = network4.forward(X_test)\n",
    "    \n",
    "    # determine class predictions from posteriors by winner-takes-all rule\n",
    "    predicted_classes = np.argmax(predicted_posteriors, axis=1)\n",
    "    predicted_classes2 = np.argmax(predicted_posteriors2, axis=1)\n",
    "    predicted_classes3 = np.argmax(predicted_posteriors3, axis=1)\n",
    "    predicted_classes4 = np.argmax(predicted_posteriors4, axis=1)\n",
    "    \n",
    "    # compute and output the error rate of predicted_classes\n",
    "    error_rate = 1-np.sum(predicted_classes == Y_test) / len(Y_test)\n",
    "    error_rate2 = 1-np.sum(predicted_classes2 == Y_test) / len(Y_test)\n",
    "    error_rate3 = 1-np.sum(predicted_classes3 == Y_test) / len(Y_test)\n",
    "    error_rate4 = 1-np.sum(predicted_classes4 == Y_test) / len(Y_test)    \n",
    "    \n",
    "    # print error rates\n",
    "    print(\"Validation Errors:\")\n",
    "    print(\"MLP([2, 2, n_classes]):\", error_rate)\n",
    "    print(\"MLP([3, 3, n_classes]):\", error_rate2)\n",
    "    print(\"MLP([5, 5, n_classes]):\", error_rate3)\n",
    "    print(\"MLP([30, 30, n_classes]):\", error_rate4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a51dade-7724-4ce7-8ec0-44154111b5a9",
   "metadata": {},
   "source": [
    "Comparing the validation errors we can see that the network with 30 neurons has the best performance. The performance does not increase according to the number of neurons, since the network with 2 neurons seems to have a lower error rate than network with 3 neurons.\n",
    "\n",
    "This behaviour could arise from a dataset whose datapoints show a somewhat linearly separable trend (have a linear tendency maybe with some errors) on a broader level, but whose true underlying classification decision boundary lies in a deeply transfored non-linear space. Hence, when only a few linear transformations have been done (2\\*2), the prediction error is less than with intermediate nr of non-linear transformations (3\\*3), which underfits the data. Lastly, the prediction error increases again with deeper non-linear transformations of the input space (> 5\\*5)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}