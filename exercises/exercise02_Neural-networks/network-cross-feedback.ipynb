{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">For the ReLULayer you use different functions in forward and backward such as heaviside function which is helpful to calculate the gradient of ReLU function.But you implement the same method as in example.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">For the OutputLayer your functions are almost totally same as the example and are right.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">For the LinearLayer except the size of bias matrix functions are basicly same as the example. But using different size also get right answer if you do not use transpose in the calculation. So you are also right.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">For the backward part of MLP, your method is basicly right but the dimention of the Layer when you iterate the layer is wrong. Because the parameters contains in the layer before last layer. So you should start from that layer as in the example instead of the last layer.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Your evaluation part is totally right and you also compare the influence of differnt size of the input. It is very great!</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate network_1: 0.115\n",
      "error rate network_2: 0.099\n",
      "error rate network_3: 0.0\n",
      "error rate network_4: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "####################################\n",
    "\n",
    "class ReLULayer(object):\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the ReLU of the input\n",
    "        relu = np.maximum(0,input) # definition of Relu function element-wise\n",
    "        return relu\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of ReLU from upstream_gradient and the stored input\n",
    "        downstream_gradient = upstream_gradient*np.heaviside(self.input,0) # derivative of Relu is heaviside of input\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass # ReLU is parameter-free\n",
    "\n",
    "####################################\n",
    "\n",
    "class OutputLayer(object):\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the softmax of the input\n",
    "        exp_values=np.exp(input-np.max(input, axis=1, keepdims=True)) #substracting the max to preventing a overflow \n",
    "        softmax=exp_values/np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return softmax\n",
    "\n",
    "    def backward(self, predicted_posteriors, true_labels):\n",
    "        # return the loss derivative with respect to the stored inputs\n",
    "        # (use cross-entropy loss and the chain rule for softmax,\n",
    "        #  as derived in the lecture)\n",
    "        \n",
    "        #derivative of softmax is just softmax itself except for the entry representing the true label where derivative is softmax - 1\n",
    "        downstream_gradient = predicted_posteriors \n",
    "        downstream_gradient[range(len(true_labels)), true_labels] -= 1 \n",
    "        downstream_gradient = downstream_gradient/len(true_labels) \n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass # softmax is parameter-free\n",
    "\n",
    "####################################\n",
    "\n",
    "class LinearLayer(object):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.n_inputs  = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        # randomly initialize weights and intercepts\n",
    "        self.B = np.random.normal(size=(n_inputs, n_outputs)) # initialize random weight matrix with n_inputs rows and n_outputs columns\n",
    "        self.b = np.random.normal(size=(1,n_outputs)) # initialize random bias vector --> has to have same dimension as output\n",
    "\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # compute the scalar product of input and weights\n",
    "        # (these are the preactivations for the subsequent non-linear layer)\n",
    "        preactivations = np.dot(self.input, self.B) + self.b # Linear combination of input with weights as in weight matrix and bias vector added afterwards\n",
    "        return preactivations\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of the weights from\n",
    "        # upstream_gradient and the stored input\n",
    "        self.grad_b = np.sum(upstream_gradient) # derivative with respect to b is just 1 (in each entry) --> chain rule gives just sum of upstream_gradient \n",
    "        self.grad_B = np.dot(self.input.T, upstream_gradient) # since layer is linear, derivative w.r.t. the weights is just input --> Chain rule gives product between input at upstream_gradient \n",
    "        # compute the downstream gradient to be passed to the preceding layer\n",
    "        downstream_gradient = np.dot(upstream_gradient, self.B.T) # derivative of Z_l w.r.t. Z_{l-1} is just B since layer is linear with weights B\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        # update the weights by batch gradient descent\n",
    "        self.B = self.B - learning_rate * self.grad_B\n",
    "        self.b = self.b - learning_rate * self.grad_b\n",
    "\n",
    "####################################\n",
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, n_features, layer_sizes):\n",
    "        # constuct a multi-layer perceptron\n",
    "        # with ReLU activation in the hidden layers and softmax output\n",
    "        # (i.e. it predicts the posterior probability of a classification problem)\n",
    "        #\n",
    "        # n_features: number of inputs\n",
    "        # len(layer_size): number of layers\n",
    "        # layer_size[k]: number of neurons in layer k\n",
    "        # (specifically: layer_sizes[-1] is the number of classes)\n",
    "        self.n_layers = len(layer_sizes)\n",
    "        self.layers   = []\n",
    "\n",
    "        # create interior layers (linear + ReLU)\n",
    "        n_in = n_features\n",
    "        for n_out in layer_sizes[:-1]:\n",
    "            self.layers.append(LinearLayer(n_in, n_out))\n",
    "            self.layers.append(ReLULayer())\n",
    "            n_in = n_out\n",
    "\n",
    "        # create last linear layer + output layer\n",
    "        n_out = layer_sizes[-1]\n",
    "        self.layers.append(LinearLayer(n_in, n_out))\n",
    "        self.layers.append(OutputLayer(n_out))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X is a mini-batch of instances\n",
    "        batch_size = X.shape[0]\n",
    "        # flatten the other dimensions of X (in case instances are images)\n",
    "        X = X.reshape(batch_size, -1)\n",
    "\n",
    "        # compute the forward pass\n",
    "        # (implicitly stores internal activations for later backpropagation)\n",
    "        result = X\n",
    "        for layer in self.layers:\n",
    "            result = layer.forward(result)\n",
    "        return result\n",
    "\n",
    "    def backward(self, predicted_posteriors, true_classes):\n",
    "        # perform backpropagation w.r.t. the prediction for the latest mini-batch X\n",
    "        downstream_gradient = self.layers[-1].backward(predicted_posteriors, true_classes) # first step of backpropagation \n",
    "        for layer in reversed(self.layers[0:-1]): #iterate through remaining layers in reverse order (excluding the last layer)\n",
    "            downstream_gradient = layer.backward(downstream_gradient) \n",
    "\n",
    "    def update(self, X, Y, learning_rate):\n",
    "        posteriors = self.forward(X)\n",
    "        self.backward(posteriors, Y)\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "\n",
    "    def train(self, x, y, n_epochs, batch_size, learning_rate):\n",
    "        N = len(x)\n",
    "        n_batches = N // batch_size\n",
    "        for i in range(n_epochs):\n",
    "            # print(\"Epoch\", i)\n",
    "            # reorder data for every epoch\n",
    "            # (i.e. sample mini-batches without replacement)\n",
    "            permutation = np.random.permutation(N)\n",
    "\n",
    "            for batch in range(n_batches):\n",
    "                # create mini-batch\n",
    "                start = batch * batch_size\n",
    "                x_batch = x[permutation[start:start+batch_size]]\n",
    "                y_batch = y[permutation[start:start+batch_size]]\n",
    "\n",
    "                # perform one forward and backward pass and update network parameters\n",
    "                self.update(x_batch, y_batch, learning_rate)\n",
    "\n",
    "##################################\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    # set training/test set size\n",
    "    N = 2000\n",
    "\n",
    "    # create training and test data\n",
    "    X_train, Y_train = datasets.make_moons(N, noise=0.05)\n",
    "    X_test,  Y_test  = datasets.make_moons(N, noise=0.05)\n",
    "    n_features = 2\n",
    "    n_classes  = 2\n",
    "\n",
    "    # standardize features to be in [-1, 1]\n",
    "    offset  = X_train.min(axis=0)\n",
    "    scaling = X_train.max(axis=0) - offset\n",
    "    X_train = ((X_train - offset) / scaling - 0.5) * 2.0\n",
    "    X_test  = ((X_test  - offset) / scaling - 0.5) * 2.0\n",
    "\n",
    "    # set hyperparameters (play with these!) --> define 4 different networks with different layer sizes\n",
    "    layer_sizes_1 = [2, 2, n_classes]\n",
    "    layer_sizes_2 = [3, 3, n_classes]\n",
    "    layer_sizes_3 = [5, 5, n_classes]\n",
    "    layer_sizes_4 = [30, 30, n_classes]\n",
    "    n_epochs = 100\n",
    "    batch_size = 5\n",
    "    learning_rate = 0.06\n",
    "\n",
    "    # create network\n",
    "    network_1 = MLP(n_features, layer_sizes_1)\n",
    "    network_2 = MLP(n_features, layer_sizes_2)\n",
    "    network_3 = MLP(n_features, layer_sizes_3)\n",
    "    network_4 = MLP(n_features, layer_sizes_4)\n",
    "    \n",
    "    # train networks\n",
    "    network_1.train(X_train, Y_train, n_epochs, batch_size, learning_rate)\n",
    "    network_2.train(X_train, Y_train, n_epochs, batch_size, learning_rate)\n",
    "    network_3.train(X_train, Y_train, n_epochs, batch_size, learning_rate)\n",
    "    network_4.train(X_train, Y_train, n_epochs, batch_size, learning_rate)\n",
    "\n",
    "    # test\n",
    "    predicted_posteriors_1 = network_1.forward(X_test)\n",
    "    predicted_posteriors_2 = network_2.forward(X_test)\n",
    "    predicted_posteriors_3 = network_3.forward(X_test)\n",
    "    predicted_posteriors_4 = network_4.forward(X_test)\n",
    "    # determine class predictions from posteriors by winner-takes-all rule\n",
    "    predicted_classes_1 = np.argmax(predicted_posteriors_1, axis=1) # to determine winner we return index with highest value in the predicted_posteriors\n",
    "    predicted_classes_2 = np.argmax(predicted_posteriors_2, axis=1)\n",
    "    predicted_classes_3 = np.argmax(predicted_posteriors_3, axis=1)\n",
    "    predicted_classes_4 = np.argmax(predicted_posteriors_4, axis=1)\n",
    "    # compute and output the error rate of predicted_classes\n",
    "    error_rate_1 = (np.sum(predicted_classes_1 != Y_test))/(len(Y_test)) # (#wrong predictions)/(#total test instances)\n",
    "    error_rate_2 = (np.sum(predicted_classes_2 != Y_test))/(len(Y_test))\n",
    "    error_rate_3 = (np.sum(predicted_classes_3 != Y_test))/(len(Y_test))\n",
    "    error_rate_4 = (np.sum(predicted_classes_4 != Y_test))/(len(Y_test))\n",
    "    print(\"error rate network_1:\", error_rate_1)\n",
    "    print(\"error rate network_2:\", error_rate_2)\n",
    "    print(\"error rate network_3:\", error_rate_3)\n",
    "    print(\"error rate network_4:\", error_rate_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
