{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d, cross_entropy\n",
    "\n",
    "plt.rc(\"figure\", dpi=100)\n",
    "\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform images into normalized tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    # Kaiming He initialization (a good initialization is important)\n",
    "    # https://arxiv.org/abs/1502.01852\n",
    "    std = np.sqrt(2. / shape[0])\n",
    "    w = torch.randn(size=shape) * std\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "\n",
    "def rectify(x):\n",
    "    # Rectified Linear Unit (ReLU)\n",
    "    return torch.max(torch.zeros_like(x), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSprop(optim.Optimizer):\n",
    "    \"\"\"\n",
    "    This is a reduced version of the PyTorch internal RMSprop optimizer\n",
    "    It serves here as an example\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.5, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # state initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(grad, avg, value=-group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the neural network\n",
    "def model(x, w_h, w_h2, w_o):\n",
    "    h = rectify(x @ w_h)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Mean Train Loss: 3.96e-01\n",
      "Mean Test Loss:  1.97e-01\n",
      "Epoch: 10\n",
      "Mean Train Loss: 1.42e-01\n",
      "Mean Test Loss:  2.58e-01\n",
      "Epoch: 20\n",
      "Mean Train Loss: 1.01e-01\n",
      "Mean Test Loss:  3.08e-01\n",
      "Epoch: 30\n",
      "Mean Train Loss: 7.46e-02\n",
      "Mean Test Loss:  6.31e-01\n",
      "Epoch: 40\n",
      "Mean Train Loss: 4.82e-02\n",
      "Mean Test Loss:  6.37e-01\n",
      "Epoch: 50\n",
      "Mean Train Loss: 3.92e-02\n",
      "Mean Test Loss:  6.22e-01\n",
      "Epoch: 60\n",
      "Mean Train Loss: 2.67e-02\n",
      "Mean Test Loss:  8.37e-01\n",
      "Epoch: 70\n",
      "Mean Train Loss: 2.16e-02\n",
      "Mean Test Loss:  7.67e-01\n",
      "Epoch: 80\n",
      "Mean Train Loss: 2.41e-02\n",
      "Mean Test Loss:  8.72e-01\n",
      "Epoch: 90\n",
      "Mean Train Loss: 7.05e-03\n",
      "Mean Test Loss:  8.99e-01\n",
      "Epoch: 100\n",
      "Mean Train Loss: 4.29e-03\n",
      "Mean Test Loss:  9.54e-01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x29558a3eb50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize weights\n",
    "\n",
    "# input shape is (B, 784)\n",
    "w_h = init_weights((784, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_h2 = init_weights((625, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_o = init_weights((625, 10))\n",
    "# output shape is (B, 10)\n",
    "\n",
    "optimizer = RMSprop(params=[w_h, w_h2, w_o])\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for epoch in range(n_epochs + 1):\n",
    "    train_loss_this_epoch = []\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        x, y = batch\n",
    "\n",
    "        # our model requires flattened input\n",
    "        x = x.reshape(batch_size, 784)\n",
    "        # feed input through model\n",
    "        noise_py_x = model(x, w_h, w_h2, w_o)\n",
    "\n",
    "        # reset the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "\n",
    "        train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "    # test periodically\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(f\"Mean Train Loss: {train_loss[-1]:.2e}\")\n",
    "        test_loss_this_epoch = []\n",
    "\n",
    "        # no need to compute gradients for validation\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(test_dataloader):\n",
    "                x, y = batch\n",
    "                x = x.reshape(batch_size, 784)\n",
    "                noise_py_x = model(x, w_h, w_h2, w_o)\n",
    "\n",
    "                loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        test_loss.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "        print(f\"Mean Test Loss:  {test_loss[-1]:.2e}\")\n",
    "\n",
    "plt.plot(np.arange(n_epochs + 1), train_loss, label=\"Train\")\n",
    "plt.plot(np.arange(1, n_epochs + 2, 10), test_loss, label=\"Test\")\n",
    "plt.title(\"Train and Test Loss over Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout (X, p_drop =0.5) :\n",
    "    if 0 < p_drop < 1:\n",
    "        # randomly mask elements of X based on binomial distribution\n",
    "        mask = np.random.binomial(1, p=p_drop, size=X.shape).astype(bool)\n",
    "        # copy of X is created using X.clone() to avoid modifying the original tensor\n",
    "        X_drop = X.clone()\n",
    "        X_drop[mask] = 0\n",
    "        X_drop /= (1 - p_drop)\n",
    "        return X_drop\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_model (x, w_h , w_h2 , w_o , p_drop_input , p_drop_hidden ):\n",
    "\n",
    "    \n",
    "   # apply dropout to the input layer\n",
    "    x = dropout(x, p_drop_input)\n",
    "\n",
    "    # first hidden layer\n",
    "    h = rectify(x @ w_h)\n",
    "    # apply dropout to the first hidden layer\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "\n",
    "    # second hidden layer\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    # apply dropout to the second hidden layer\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "\n",
    "    # output layer\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Mean Train Loss: 7.40e-01\n",
      "Mean Test Loss:  4.36e-01\n",
      "Epoch: 10\n",
      "Mean Train Loss: 6.95e-01\n",
      "Mean Test Loss:  7.75e-01\n",
      "Epoch: 20\n",
      "Mean Train Loss: 8.49e-01\n",
      "Mean Test Loss:  1.06e+00\n",
      "Epoch: 30\n",
      "Mean Train Loss: 9.61e-01\n",
      "Mean Test Loss:  1.08e+00\n",
      "Epoch: 40\n",
      "Mean Train Loss: 1.06e+00\n",
      "Mean Test Loss:  1.42e+00\n",
      "Epoch: 50\n",
      "Mean Train Loss: 1.11e+00\n",
      "Mean Test Loss:  1.54e+00\n",
      "Epoch: 60\n",
      "Mean Train Loss: 1.18e+00\n",
      "Mean Test Loss:  1.40e+00\n",
      "Epoch: 70\n",
      "Mean Train Loss: 1.28e+00\n",
      "Mean Test Loss:  1.56e+00\n",
      "Epoch: 80\n",
      "Mean Train Loss: 1.35e+00\n",
      "Mean Test Loss:  2.01e+00\n",
      "Epoch: 90\n",
      "Mean Train Loss: 1.39e+00\n",
      "Mean Test Loss:  1.58e+00\n",
      "Epoch: 100\n",
      "Mean Train Loss: 1.39e+00\n",
      "Mean Test Loss:  1.85e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x24d17ea5d10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set dropout probabilities\n",
    "p_drop_input = 0.2\n",
    "p_drop_hidden = 0.5\n",
    "\n",
    "# initialize weights\n",
    "\n",
    "# input shape is (B, 784)\n",
    "w_h = init_weights((784, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_h2 = init_weights((625, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_o = init_weights((625, 10))\n",
    "# output shape is (B, 10)\n",
    "\n",
    "optimizer = RMSprop(params=[w_h, w_h2, w_o])\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for epoch in range(n_epochs + 1):\n",
    "    train_loss_this_epoch = []\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        x, y = batch\n",
    "\n",
    "        # our model requires flattened input\n",
    "        x = x.reshape(batch_size, 784)\n",
    "        # feed input through model\n",
    "        noise_py_x = dropout_model(x, w_h, w_h2, w_o,p_drop_input,p_drop_hidden)\n",
    "\n",
    "        # reset the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "\n",
    "        train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "    # test periodically\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(f\"Mean Train Loss: {train_loss[-1]:.2e}\")\n",
    "        test_loss_this_epoch = []\n",
    "\n",
    "        # no need to compute gradients for validation\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(test_dataloader):\n",
    "                x, y = batch\n",
    "                x = x.reshape(batch_size, 784)\n",
    "                noise_py_x = dropout_model(x, w_h, w_h2, w_o,p_drop_input,p_drop_hidden)\n",
    "\n",
    "                loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        test_loss.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "        print(f\"Mean Test Loss:  {test_loss[-1]:.2e}\")\n",
    "\n",
    "plt.plot(np.arange(n_epochs + 1), train_loss, label=\"Train\")\n",
    "plt.plot(np.arange(1, n_epochs + 2, 10), test_loss, label=\"Test\")\n",
    "plt.title(\"Train and Test Loss over Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3Parametric Relu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PRelu (X,a):\n",
    "    return np.maximum(0,X)+np.minimum(0,a*X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  -0.5]\n"
     ]
    }
   ],
   "source": [
    "#test PRelu function \n",
    "x=np.array([1, -1])\n",
    "a=0.5\n",
    "x_new=PRelu(x,a)\n",
    "print(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Mean Train Loss: 5.76e-01\n",
      "Mean Test Loss:  3.80e-01\n",
      "Epoch: 10\n",
      "Mean Train Loss: 5.26e-01\n",
      "Mean Test Loss:  5.61e-01\n",
      "Epoch: 20\n",
      "Mean Train Loss: 6.36e-01\n",
      "Mean Test Loss:  8.54e-01\n",
      "Epoch: 30\n",
      "Mean Train Loss: 7.48e-01\n",
      "Mean Test Loss:  8.64e-01\n",
      "Epoch: 40\n",
      "Mean Train Loss: 7.85e-01\n",
      "Mean Test Loss:  1.44e+00\n",
      "Epoch: 50\n",
      "Mean Train Loss: 8.31e-01\n",
      "Mean Test Loss:  1.36e+00\n",
      "Epoch: 60\n",
      "Mean Train Loss: 8.92e-01\n",
      "Mean Test Loss:  1.52e+00\n",
      "Epoch: 70\n",
      "Mean Train Loss: 9.19e-01\n",
      "Mean Test Loss:  1.29e+00\n",
      "Epoch: 80\n",
      "Mean Train Loss: 9.92e-01\n",
      "Mean Test Loss:  1.49e+00\n",
      "Epoch: 90\n",
      "Mean Train Loss: 9.98e-01\n",
      "Mean Test Loss:  1.70e+00\n",
      "Epoch: 100\n",
      "Mean Train Loss: 9.96e-01\n",
      "Mean Test Loss:  1.76e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x21b7757e810>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set dropout probabilities\n",
    "p_drop_input = 0.2\n",
    "p_drop_hidden = 0.5\n",
    "\n",
    "#set a\n",
    "a=0.5\n",
    "\n",
    "# initialize weights\n",
    "\n",
    "# input shape is (B, 784)\n",
    "w_h = init_weights((784, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_h2 = init_weights((625, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_o = init_weights((625, 10))\n",
    "# output shape is (B, 10)\n",
    "\n",
    "optimizer = RMSprop(params=[w_h, w_h2, w_o])\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for epoch in range(n_epochs + 1):\n",
    "    train_loss_this_epoch = []\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        x, y = batch\n",
    "\n",
    "        # our model requires flattened input\n",
    "        x = x.reshape(batch_size, 784)\n",
    "        # feed input through model\n",
    "        \n",
    "        #PRelu\n",
    "        x=PRelu(x,a)\n",
    "        \n",
    "        noise_py_x = dropout_model(x, w_h, w_h2, w_o,p_drop_input,p_drop_hidden)\n",
    "\n",
    "        # reset the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "\n",
    "        train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "    # test periodically\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(f\"Mean Train Loss: {train_loss[-1]:.2e}\")\n",
    "        test_loss_this_epoch = []\n",
    "\n",
    "        # no need to compute gradients for validation\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(test_dataloader):\n",
    "                x, y = batch\n",
    "                x = x.reshape(batch_size, 784)\n",
    "                \n",
    "                #PRelu\n",
    "                x=PRelu(x,a)\n",
    "                \n",
    "                noise_py_x = dropout_model(x, w_h, w_h2, w_o,p_drop_input,p_drop_hidden)\n",
    "\n",
    "                loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        test_loss.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "        print(f\"Mean Test Loss:  {test_loss[-1]:.2e}\")\n",
    "\n",
    "plt.plot(np.arange(n_epochs + 1), train_loss, label=\"Train\")\n",
    "plt.plot(np.arange(1, n_epochs + 2, 10), test_loss, label=\"Test\")\n",
    "plt.title(\"Train and Test Loss over Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the convolutional neural network\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Linear(400, 120)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(84, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Hyperparameters\n",
    "\n",
    "\n",
    "model = LeNet5(num_classes).to(device)\n",
    "\n",
    "#Setting the loss function\n",
    "cost = nn.CrossEntropyLoss()\n",
    "\n",
    "#Setting the optimizer with the model parameters and learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#this is defined to print how many steps are remaining when training\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = cost(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 400 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "  \n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch .nn. functional import conv2d , max_pool2d\n",
    "\n",
    "convolutional_layer = rectify ( conv2d ( previous_layer , weightvector ))\n",
    "# reduces (2 ,2) window to 1 pixel\n",
    "subsampling_layer = max_pool_2d ( convolutional_layer , (2 , 2) )\n",
    "out_layer = dropout ( subsample_layer , p_drop_input )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the neural network\n",
    "def LeNetmodel(x, w_h, w_h2, w_o,p_drop_input , p_drop_hidden):\n",
    "    x=x.reshape ( -1 , 1, 28 , 28) \n",
    "    \n",
    "    #h\n",
    "    convolutional_layer = rectify ( conv2d ( x , w_h ))\n",
    "    subsampling_layer = max_pool2d ( convolutional_layer , (2 , 2) )\n",
    "    out_layer = dropout ( subsampling_layer , p_drop_input )\n",
    "    \n",
    "    convolutional_layer = rectify ( conv2d ( out_layer , w_h2 ))\n",
    "    subsampling_layer = max_pool2d ( convolutional_layer , (2 , 2) )\n",
    "    out_layer = dropout ( subsampling_layer , p_drop_input )\n",
    "    \n",
    "    convolutional_layer = rectify ( conv2d ( out_layer , w_o ))\n",
    "    subsampling_layer = max_pool2d ( convolutional_layer , (2 , 2) )\n",
    "    out_layer = dropout ( subsampling_layer , p_drop_input )\n",
    "    \n",
    "    #h2\n",
    "    #print(out_layer.shape)#[100,128,1,1]\n",
    "    h=out_layer.reshape(100,128)\n",
    "    old_w_h2 = init_weights (( 128 , 625) )\n",
    "    old_w_o = init_weights((625, 10))\n",
    "    \n",
    "    \n",
    "    #h = rectify(x @ w_h)\n",
    "    h2 = rectify(h @ old_w_h2)\n",
    "    pre_softmax = h2 @ old_w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Mean Train Loss: 3.15e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 10\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 20\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 30\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n",
      "Epoch: 40\n",
      "Mean Train Loss: 2.30e+00\n",
      "Mean Test Loss:  2.30e+00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set dropout probabilities\n",
    "p_drop_input = 0.2\n",
    "p_drop_hidden = 0.5\n",
    "\n",
    "#set a\n",
    "a=0.5\n",
    "\n",
    "# initialize weights\n",
    "\n",
    "# input shape is (B, 784)\n",
    "#w_h = init_weights((784, 625))\n",
    "# hidden layer with 625 neurons\n",
    "#w_h2 = init_weights((625, 625))\n",
    "# hidden layer with 625 neurons\n",
    "#w_o = init_weights((625, 10))\n",
    "# output shape is (B, 10)\n",
    "#init_weights ((f, pic_in , k_x , k_y ))\n",
    "w1=init_weights ((32, 1 , 5 , 5 ))\n",
    "w2=init_weights ((64, 32 , 5 , 5 ))\n",
    "w3=init_weights ((128, 64 , 2 , 2 ))\n",
    "\n",
    "optimizer = RMSprop(params=[w1, w2, w3])\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for epoch in range(n_epochs + 1):\n",
    "    train_loss_this_epoch = []\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        x, y = batch\n",
    "        \n",
    "        # our model requires flattened input\n",
    "        x = x.reshape(batch_size, 784)\n",
    "        # feed input through model\n",
    "        \n",
    "        #PRelu\n",
    "        x=PRelu(x,a)\n",
    "        \n",
    "        noise_py_x = LeNetmodel(x, w1, w2, w3,p_drop_input,p_drop_hidden)\n",
    "\n",
    "        # reset the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "\n",
    "        train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "    # test periodically\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(f\"Mean Train Loss: {train_loss[-1]:.2e}\")\n",
    "        test_loss_this_epoch = []\n",
    "\n",
    "        # no need to compute gradients for validation\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(test_dataloader):\n",
    "                x, y = batch\n",
    "                x = x.reshape(batch_size, 784)\n",
    "                \n",
    "                #PRelu\n",
    "                x=PRelu(x,a)\n",
    "                \n",
    "                noise_py_x = LeNetmodel(x, w1, w2, w3,p_drop_input,p_drop_hidden)\n",
    "\n",
    "                loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        test_loss.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "        print(f\"Mean Test Loss:  {test_loss[-1]:.2e}\")\n",
    "\n",
    "plt.plot(np.arange(n_epochs + 1), train_loss, label=\"Train\")\n",
    "plt.plot(np.arange(1, n_epochs + 2, 10), test_loss, label=\"Test\")\n",
    "plt.title(\"Train and Test Loss over Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_homework",
   "language": "python",
   "name": "ml_homework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
